{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preperation\n",
    "\n",
    "To prepare our dataset for use we will need to do some cleaning and normalizing of the pictures. \n",
    "First, we will use the bounding box information we gathered earlier so that our photos will only\n",
    "have dogs in them. Then we will resize the images so that they are uniform in addition to \n",
    "normalizing the colors. \n",
    "\n",
    "After we have created our high quality dog images we will do some image augmentation to help \n",
    "under represented breeds (n < 200) and breed categories (wild dogs and Foundation Stock Service).\n",
    "We will do this using \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>breed_name</th>\n",
       "      <th>folder</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>pose</th>\n",
       "      <th>truncated</th>\n",
       "      <th>difficult</th>\n",
       "      <th>group_x</th>\n",
       "      <th>group_y</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n02085620_10074.jpg</td>\n",
       "      <td>chihuahua</td>\n",
       "      <td>n02085620-Chihuahua</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>276</td>\n",
       "      <td>498</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n02085620_10131.jpg</td>\n",
       "      <td>chihuahua</td>\n",
       "      <td>n02085620-Chihuahua</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>393</td>\n",
       "      <td>493</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n02085620_10621.jpg</td>\n",
       "      <td>chihuahua</td>\n",
       "      <td>n02085620-Chihuahua</td>\n",
       "      <td>142</td>\n",
       "      <td>43</td>\n",
       "      <td>335</td>\n",
       "      <td>250</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n02085620_1073.jpg</td>\n",
       "      <td>chihuahua</td>\n",
       "      <td>n02085620-Chihuahua</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>312</td>\n",
       "      <td>498</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n02085620_10976.jpg</td>\n",
       "      <td>chihuahua</td>\n",
       "      <td>n02085620-Chihuahua</td>\n",
       "      <td>90</td>\n",
       "      <td>104</td>\n",
       "      <td>242</td>\n",
       "      <td>452</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "      <td>Toy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename breed_name               folder  xmin  ymin  xmax  \\\n",
       "0  n02085620_10074.jpg  chihuahua  n02085620-Chihuahua    25    10   276   \n",
       "1  n02085620_10131.jpg  chihuahua  n02085620-Chihuahua    49     9   393   \n",
       "2  n02085620_10621.jpg  chihuahua  n02085620-Chihuahua   142    43   335   \n",
       "3   n02085620_1073.jpg  chihuahua  n02085620-Chihuahua     0    27   312   \n",
       "4  n02085620_10976.jpg  chihuahua  n02085620-Chihuahua    90   104   242   \n",
       "\n",
       "   ymax         pose  truncated  difficult group_x group_y group  \n",
       "0   498  Unspecified          0          0     Toy     Toy   Toy  \n",
       "1   493  Unspecified          0          0     Toy     Toy   Toy  \n",
       "2   250  Unspecified          0          0     Toy     Toy   Toy  \n",
       "3   498  Unspecified          0          0     Toy     Toy   Toy  \n",
       "4   452  Unspecified          0          0     Toy     Toy   Toy  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('resources/csv/dog_annotations_with_groups.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cropping complete! 1 issues logged.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "error_log = []\n",
    "cropped_folder = 'resources/stanford-dogs-dataset/cropped-images'\n",
    "image_folder = 'resources/stanford-dogs-dataset/images'\n",
    "breed_issues = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    breed_name = row[\"breed_name\"]\n",
    "    folder_name = row[\"folder\"].strip()  # Strip newlines & spaces\n",
    "    filename = row[\"filename\"].strip()  # Ensure filename is not empty\n",
    "\n",
    "    # Check if filename is empty or incorrect\n",
    "    if not filename or filename == \"%s.jpg\":\n",
    "        # print(f\"‚ö†Ô∏è Invalid filename for {breed_name} in {folder_name}\")\n",
    "        error_log.append(f\"Invalid filename for {breed_name} in {folder_name}\")\n",
    "        continue  # Skip this entry\n",
    "\n",
    "    img_path = os.path.join(image_folder, folder_name, filename)\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        # print(f\"‚ö†Ô∏è Missing file: {img_path}\")\n",
    "        error_log.append(img_path)\n",
    "        continue  # Skip missing images\n",
    "\n",
    "    try: # Run of the mill image cropping routine\n",
    "        # Open image\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Crop\n",
    "        xmin, ymin, xmax, ymax = row[\"xmin\"], row[\"ymin\"], row[\"xmax\"], row[\"ymax\"]\n",
    "        cropped_img = img.crop((xmin, ymin, xmax, ymax))\n",
    "\n",
    "        # Ensure breed folder exists\n",
    "        breed_cropped_path = os.path.join(cropped_folder, breed_name)\n",
    "        os.makedirs(breed_cropped_path, exist_ok=True)\n",
    "\n",
    "        # Save cropped image\n",
    "        save_path = os.path.join(breed_cropped_path, filename)\n",
    "        cropped_img.save(save_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log.append(f\"{img_path} - {e}\")\n",
    "\n",
    "# Save error log\n",
    "with open(\"missing_images_log.txt\", \"w\") as f:\n",
    "    for entry in error_log:\n",
    "        f.write(f\"{entry}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Cropping complete! {len(error_log)} issues logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Breeds in dataset but missing from image folders: set()\n",
      "üìå Extra breed folders that don't match dataset: set()\n"
     ]
    }
   ],
   "source": [
    "expected_breeds = set(df[\"breed_name\"].unique())\n",
    "actual_breeds = set(os.listdir(cropped_folder))\n",
    "\n",
    "# Check which breeds are missing from the folders\n",
    "missing_folders = expected_breeds - actual_breeds\n",
    "extra_folders = actual_breeds - expected_breeds\n",
    "\n",
    "print(\"üìå Breeds in dataset but missing from image folders:\", missing_folders)\n",
    "print(\"üìå Extra breed folders that don't match dataset:\", extra_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20579 cropped images\n",
      "There are 1547 missing images.\n"
     ]
    }
   ],
   "source": [
    "total_imgs = 0\n",
    "for breed in os.listdir(cropped_folder):\n",
    "    breed_path = os.path.join(cropped_folder, breed)\n",
    "    num_images = len(os.listdir(breed_path))\n",
    "    total_imgs += num_images\n",
    "\n",
    "print(f\"There are {total_imgs} cropped images\")\n",
    "print(f\"There are {len(df['breed_name']) - total_imgs} missing images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we are missing 10% of our data for some reason. I am dealing with that later. \n",
    "## Image Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20579 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define normalization and augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=15,  # Rotate up to ¬±15 degrees\n",
    "    width_shift_range=0.1,  # Shift image width by 10%\n",
    "    height_shift_range=0.1,  # Shift image height by 10%\n",
    "    shear_range=0.1,  # Shear transform\n",
    "    zoom_range=0.2,  # Zoom in/out by up to 20%\n",
    "    horizontal_flip=True,  # Flip images horizontally\n",
    "    brightness_range=[0.8, 1.2],  # Random brightness adjustment\n",
    "    fill_mode='nearest'  # Fill in missing pixels after transformation\n",
    ")\n",
    "\n",
    "# Apply transformations to dataset\n",
    "train_generator = datagen.flow_from_directory(cropped_folder, target_size=(128,128), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20579 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "def resize_with_padding(image, target_size=(128, 128)):\n",
    "    \"\"\"Resize an image while keeping aspect ratio and adding padding.\"\"\"\n",
    "    image = np.array(image)  # Ensure it's a NumPy array\n",
    "    image = (image * 255).astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "    img = Image.fromarray(image)  # Convert to PIL Image\n",
    "    img = ImageOps.fit(img, target_size, method=Image.Resampling.LANCZOS, centering=(0.5, 0.5))\n",
    "\n",
    "    return np.array(img)  # Convert back to NumPy array\n",
    "\n",
    "\n",
    "# Define a custom preprocessing function for ImageDataGenerator\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Resize with padding and normalize pixel values.\"\"\"\n",
    "    image = resize_with_padding(image)  # Resize and pad\n",
    "    image = image / 255.0  # Normalize pixels to [0,1]\n",
    "    return image\n",
    "\n",
    "# Apply it in ImageDataGenerator\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    cropped_folder,  # Your dataset path\n",
    "    target_size=(128, 128),  # The final target size\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: (64, 128, 128, 3)\n",
      "Label batch shape: (64, 120)\n",
      "Pixel range: min=0.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "batch_images, batch_labels = next(train_generator)\n",
    "\n",
    "print(f\"Image batch shape: {batch_images.shape}\")  # Should be (batch_size, 128, 128, 3)\n",
    "print(f\"Label batch shape: {batch_labels.shape}\")  # Should match the number of classes\n",
    "print(f\"Pixel range: min={batch_images.min()}, max={batch_images.max()}\")  # Should be between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16507 images belonging to 120 classes.\n",
      "Found 4072 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    "    validation_split=0.2  # 20% validation\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    cropped_folder,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=64,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    cropped_folder,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=64,\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m      3\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m      4\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      5\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m      6\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m128\u001b[39m, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      7\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m      8\u001b[0m     layers\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[0;32m      9\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m512\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     10\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m116\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 116 classes\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ])\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()  \u001b[38;5;66;03m# Print model structure\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(116, activation='softmax')  # 116 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()  # Print model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "251/251 [==============================] - 149s 591ms/step - loss: 4.5956 - accuracy: 0.0290 - val_loss: 4.3743 - val_accuracy: 0.0486\n",
      "Epoch 2/10\n",
      "251/251 [==============================] - 154s 613ms/step - loss: 3.9756 - accuracy: 0.0981 - val_loss: 4.0302 - val_accuracy: 0.0896\n",
      "Epoch 3/10\n",
      "251/251 [==============================] - 156s 621ms/step - loss: 3.2242 - accuracy: 0.2316 - val_loss: 3.9100 - val_accuracy: 0.1167\n",
      "Epoch 4/10\n",
      "251/251 [==============================] - 174s 692ms/step - loss: 2.0353 - accuracy: 0.4866 - val_loss: 4.6806 - val_accuracy: 0.1172\n",
      "Epoch 5/10\n",
      "251/251 [==============================] - 177s 706ms/step - loss: 0.8221 - accuracy: 0.7858 - val_loss: 6.6313 - val_accuracy: 0.1142\n",
      "Epoch 6/10\n",
      "251/251 [==============================] - 241s 959ms/step - loss: 0.2708 - accuracy: 0.9300 - val_loss: 8.4326 - val_accuracy: 0.1003\n",
      "Epoch 7/10\n",
      "251/251 [==============================] - 179s 709ms/step - loss: 0.1442 - accuracy: 0.9667 - val_loss: 9.2541 - val_accuracy: 0.1074\n",
      "Epoch 8/10\n",
      "251/251 [==============================] - 150s 597ms/step - loss: 0.0942 - accuracy: 0.9783 - val_loss: 9.6591 - val_accuracy: 0.1021\n",
      "Epoch 9/10\n",
      "251/251 [==============================] - 140s 558ms/step - loss: 0.0509 - accuracy: 0.9901 - val_loss: 10.3470 - val_accuracy: 0.0972\n",
      "Epoch 10/10\n",
      "251/251 [==============================] - 138s 549ms/step - loss: 0.0419 - accuracy: 0.9913 - val_loss: 10.4139 - val_accuracy: 0.1094\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10  # Adjust as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 13s 210ms/step - loss: 10.4139 - accuracy: 0.1094\n",
      "Validation Accuracy: 10.94%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "model.save(\"dog_breed_classifier.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cutie_zone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
